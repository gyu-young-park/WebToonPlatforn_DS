# -*- coding: utf-8 -*-
"""tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmdYXZ9A3b-HcKYF2ZhuonrkksT8oFiF
"""

# 토크나이저 define
from collections import Counter, defaultdict

SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']


# 음절 단위로 vocab을 형성한다. 
class CharTokenizer(object):
    def __name__(self):
      
  __main__.CharTokenizer
    def __init__(self, i2c):
        self.init(i2c)

    def __len__(self):
        return len(self.vocab)

    def __call__(self, sent):
        return [self.vocab[c] for c in sent]

    def init(self, i2c):
        self.i2c = i2c
        self.vocab = defaultdict(int)
        self.vocab.update({c: i for i, c in enumerate(i2c)})

    @classmethod
    def from_strings(cls, strings, vocab_size): # 전체 dataset으로 vocab을 만들어 준다.
        char_counter = Counter()
        for x in strings:
            char_counter.update(x)
        i2c = SPECIAL_TOKENS # vocab의 맨 앞 4개는 special 토큰으로 채워줌
        i2c += [c for c, _ in char_counter.most_common(vocab_size - len(SPECIAL_TOKENS))] # 나머지는 dataset에 있는 단어로 채움 
        return cls(i2c) # init을 호출하면서 return

def get_tokenizer():
  # 전체 문자열 가져오기
  df = pd.read_csv('webtoon_comments_labeled.csv',encoding='utf-16')
  comment = df['comment'].tolist()
  # tokneizer 생성 : 총 char수에서 217개를 뺀 수를 vocab size로 지정한다.(자주 등장하는 cahr만 사용하기 위함)
  char_set = set()
  for cmt in comment:
    for char in cmt:
      char_set.add(char)

  tokenizer = CharTokenizer([])
  tokenizer= CharTokenizer.from_strings(comment, len(char_set)-217)
  return tokenizer