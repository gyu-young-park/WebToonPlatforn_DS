# -*- coding: utf-8 -*-
"""Naver_comment_text_to_tensor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmdYXZ9A3b-HcKYF2ZhuonrkksT8oFiF
"""

# 토크나이저 define
from collections import Counter, defaultdict

SPECIAL_TOKENS = ['<unk>', '<pad>', '<sos>', '<eos>']


# 음절 단위로 vocab을 형성한다. 
class CharTokenizer(object):
    def __init__(self, i2c):
        self.init(i2c)

    def __len__(self):
        return len(self.vocab)

    def __call__(self, sent):
        return [self.vocab[c] for c in sent]

    def init(self, i2c):
        self.i2c = i2c
        self.vocab = defaultdict(int)
        self.vocab.update({c: i for i, c in enumerate(i2c)})

    @classmethod
    def from_strings(cls, strings, vocab_size): # 전체 dataset으로 vocab을 만들어 준다.
        char_counter = Counter()
        for x in strings:
            char_counter.update(x)
        i2c = SPECIAL_TOKENS # vocab의 맨 앞 4개는 special 토큰으로 채워줌
        i2c += [c for c, _ in char_counter.most_common(vocab_size - len(SPECIAL_TOKENS))] # 나머지는 dataset에 있는 단어로 채움 
        return cls(i2c) # init을 호출하면서 return

def get_tokenizer():
  import pandas as pd
  # 전체 문자열 가져오기
  df = pd.read_csv('webtoon_comments_labeled.csv',encoding='utf-16')
  comment = df['comment'].tolist()
  # tokneizer 생성 : 총 char수에서 217개를 뺀 수를 vocab size로 지정한다.(자주 등장하는 cahr만 사용하기 위함)
  char_set = set()
  for cmt in comment:
    for char in cmt:
      char_set.add(char)

  tokenizer = CharTokenizer([])
  tokenizer= CharTokenizer.from_strings(comment, len(char_set)-217)
  return tokenizer

def text_to_data(tokenizer, comments, src_max_seq_length=128):
  if not isinstance(comments,list):
    print("comments is not list.")
    return None
  import torch
  src_padded = []
  src_mask = []
  for comment in comments:
    comment = [data for data in comment]
    len_comment = len(comment)
    comment = comment + [tokenizer.vocab['<pad>'] for _ in range(128-len(comment))]
    data = tokenizer.__call__(comment)
    data = data
    src_padded.append(data)
    src_mask.append([1]*len_comment + [tokenizer.vocab['<pad>']]*(src_max_seq_length-len_comment))
  src_padded = torch.tensor(src_padded).t().contiguous()
  src_mask = torch.tensor(src_mask).bool()
  return src_padded,src_mask

def infer(model,data=None):
  import json
  if data is None:
    print("You got wrong data : None")
    return None
  src, src_mask = data
  output = model_unit(src=src.to(device),src_key_padding_mask=~src_mask.to(device))
  _, predicted = torch.max(output.data, 1)
  labels = json.dumps({"labels" : predicted.tolist()})
  return labels
